{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13351,"databundleVersionId":324297,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Train.csv')\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(df['Text']).toarray()  \npca = PCA(n_components=2)\nX_pca = pca.fit_transform(features)\n\nfig_2d = px.scatter(\n    x=X_pca[:, 0], y=X_pca[:, 1], \n    color=df['Category'],  \n    labels={'x': 'Principal Component 1', 'y': 'Principal Component 2'},\n    title=\"PCA Projection: High Dimensionality to 2D\"\n)\nfig_2d.show()\n\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(features)\n\n# 3D Plot\nfig_3d = px.scatter_3d(\n    x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2], \n    color=df['Category'],  \n    labels={'x': 'Principal Component 1', 'y': 'Principal Component 2', 'z': 'Principal Component 3'},\n    title=\"PCA Projection: High Dimensionality to 3D\"\n)\nfig_3d.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T07:48:42.254009Z","iopub.execute_input":"2025-03-28T07:48:42.254469Z","iopub.status.idle":"2025-03-28T07:48:44.432464Z","shell.execute_reply.started":"2025-03-28T07:48:42.254436Z","shell.execute_reply":"2025-03-28T07:48:44.431327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Train.csv')\n\n# Check for missing values in the dataset\nmissing_values = df.isnull().sum()\nprint(missing_values)\nmissing_percentage = (df.isnull().sum() / len(df)) * 100\nprint(missing_percentage)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T07:48:44.434134Z","iopub.execute_input":"2025-03-28T07:48:44.434571Z","iopub.status.idle":"2025-03-28T07:48:44.487447Z","shell.execute_reply.started":"2025-03-28T07:48:44.434512Z","shell.execute_reply":"2025-03-28T07:48:44.486575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ntrain_df = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Train.csv')\n\ntexts = train_df['Text']\n\ntfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n\nX = tfidf.fit_transform(texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T07:48:44.489087Z","iopub.execute_input":"2025-03-28T07:48:44.489419Z","iopub.status.idle":"2025-03-28T07:48:45.050990Z","shell.execute_reply.started":"2025-03-28T07:48:44.489391Z","shell.execute_reply":"2025-03-28T07:48:45.049860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Tune max_features for dimension reduction\nX_tfidf = vectorizer.fit_transform(train_df['Text'])\n\ninertia = []\ncluster_range = range(1, 11)\n\nfor num_clusters in cluster_range:\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(X_tfidf)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(cluster_range, inertia, marker='o')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.show()\n\noptimal_clusters = 3  \nkmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\ntrain_df['Cluster'] = kmeans.fit_predict(X_tfidf)\n\nprint(train_df[['Text', 'Cluster']].head())\n\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T07:48:45.052440Z","iopub.execute_input":"2025-03-28T07:48:45.052861Z","iopub.status.idle":"2025-03-28T07:48:51.045486Z","shell.execute_reply.started":"2025-03-28T07:48:45.052823Z","shell.execute_reply":"2025-03-28T07:48:51.043418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\n\nagg_clust = AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='average')\ntrain_df['Cluster'] = agg_clust.fit_predict(X_tfidf.toarray())\n\nsch.dendrogram(sch.linkage(X_tfidf.toarray(), method='ward'))\nplt.title('Dendrogram for Agglomerative Clustering')\nplt.show()\n\nprint(train_df[['Text', 'Cluster']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T07:48:51.046217Z","iopub.execute_input":"2025-03-28T07:48:51.046499Z","iopub.status.idle":"2025-03-28T07:49:14.202029Z","shell.execute_reply.started":"2025-03-28T07:48:51.046474Z","shell.execute_reply":"2025-03-28T07:49:14.201004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndocument_idx = 150  \ninput_document_tfidf = X_tfidf[document_idx]\n\nsimilarities = cosine_similarity(input_document_tfidf, X_tfidf)\n\nsimilar_docs_idx = similarities.argsort()[0][-6:-1]\n\nprint(\"Most similar articles to the selected document:\")\nfor idx in similar_docs_idx:\n    print(f\"\\nArticle (Cluster {train_df.iloc[idx]['Cluster']}): {train_df.iloc[idx]['Text']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T08:00:41.296922Z","iopub.execute_input":"2025-03-28T08:00:41.297404Z","iopub.status.idle":"2025-03-28T08:00:41.313993Z","shell.execute_reply.started":"2025-03-28T08:00:41.297372Z","shell.execute_reply":"2025-03-28T08:00:41.312732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This dataset was tricky to work with as the multidimensional nature of full text documents made it less precise to guage similarities between documents in clusters. The 3D PCA graph at least gave me an initial guide that showed that the categories were indeed distinct, although they unfortunately all converged at the center on both the 2d and 3d graphs which I believe is what made the delineation of the data so difficult. I tried to use the elbow method to fix the difficulties I was having with k-means clustering but the graph showed absolutely no clear plateau to cut off with. It was then that I realized a hierarchical clustering model would be much more efficient with the type of data that I was working with, and the dendrogram gave me 6 clear categories, which was different from the 5 initial categories but it was certainly a very clear separation. Although this model doesn't do what the initial challenge set out for this kind of approach doesn't lend itself to what is better with supervised learning, so I applied it to find similar articles which utilises the nature of the model well.","metadata":{}}]}